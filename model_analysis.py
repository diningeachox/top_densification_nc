# -*- coding: utf-8 -*-
"""model_analysis

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZdTw18FgTKGGZn6u0L03XvYJx9DpPnmt

Measure the Neural Collapse Properties of the topologically regularized model
"""

# Commented out IPython magic to ensure Python compatibility.
import sys
sys.path.append('/content/drive/MyDrive/TDD')
# %cd /content/drive/MyDrive/TDD
# %load svd.py
# %load ds_util_mod.py
from ds_util_mod import * 

print(sys.path)

import json
import urllib.request

import numpy as np
import matplotlib
import matplotlib.pyplot as plt

from PIL import Image

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torch.utils.data as Data

import torchvision.utils
import torchvision.datasets as dsets
import torchvision.transforms as transforms
import torchvision.transforms.functional as F
from torchvision import models
from torch.distributions import MultivariateNormal
import svd

from torchvision import datasets, transforms

from sklearn.decomposition import PCA
from scipy.sparse.linalg import svds

class LinearView(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, x):
        return x.view(x.size()[0], -1)

#Model used by Hofer et al. to train on MNIST
class SimpleCNN_MNIST(nn.Module):
    def __init__(self,
                 num_classes,
                 batch_norm):
        super().__init__()

        def activation(): return nn.LeakyReLU(0.1)

        if batch_norm:
            def bn_2d(dim): return nn.BatchNorm2d(dim)

            def bn_1d(dim): return nn.BatchNorm1d(dim)
        else:
            def bn_2d(dim): return Identity(dim)

            def bn_1d(dim): return Identity(dim)

        self.feat_ext = nn.Sequential(
            nn.Conv2d(1, 8, 3, padding=1),
            bn_2d(8),
            activation(),
            nn.MaxPool2d(2, stride=2, padding=0),
            #
            nn.Conv2d(8, 32, 3, padding=1),
            bn_2d(32),
            activation(),
            nn.MaxPool2d(2, stride=2, padding=0),
            #
            nn.Conv2d(32, 64, 3, padding=1),
            bn_2d(64),
            activation(),
            nn.MaxPool2d(2, stride=2, padding=0),
            #
            nn.Conv2d(64, 128, 3, padding=1),
            bn_2d(128),
            activation(),
            nn.MaxPool2d(2, stride=2, padding=0),
            LinearView(),
        )

        cls = nn.Linear(128, num_classes)

        self.cls = nn.Sequential(cls)

    def forward(self, x):
        z = self.feat_ext(x)
        y_hat = self.cls(z)

        return y_hat

class SimpleCNN13(nn.Module):
    def __init__(self,
                 num_classes: int,
                 batch_norm: bool, 
                 drop_out: bool, 
                 cls_spectral_norm: bool,
                 final_bn: bool):
        super().__init__()

        def activation(): return nn.LeakyReLU(0.1)

        if drop_out:
            def dropout(p): return nn.Dropout(p)
        else:
            def dropout(p): return Identity()

        bn_affine = True

        if batch_norm:
            def bn_2d(dim): return nn.BatchNorm2d(dim, affine=bn_affine)

            def bn_1d(dim): return nn.BatchNorm1d(dim, affine=bn_affine)
        else:
            def bn_2d(dim): return Identity(dim)

            def bn_1d(dim): return Identity(dim)

        self.feat_ext = nn.Sequential(
            nn.Conv2d(3, 128, 3, padding=1),
            bn_2d(128),
            activation(),
            nn.Conv2d(128, 128, 3, padding=1),
            bn_2d(128),
            activation(),
            nn.Conv2d(128, 128, 3, padding=1),
            bn_2d(128),
            activation(),
            nn.MaxPool2d(2, stride=2, padding=0),
            dropout(0.5),
            #
            nn.Conv2d(128, 256, 3, padding=1),
            bn_2d(256),
            activation(),
            nn.Conv2d(256, 256, 3, padding=1),
            bn_2d(256),
            activation(),
            nn.Conv2d(256, 256, 3, padding=1),
            bn_2d(256),
            activation(),
            nn.MaxPool2d(2, stride=2, padding=0),
            dropout(0.5),
            #
            nn.Conv2d(256, 512, 3, padding=0),
            bn_2d(512),
            activation(),
            nn.Conv2d(512, 256, 1, padding=0),
            bn_2d(256),
            activation(),
            nn.Conv2d(256, 128, 1, padding=0),
            bn_2d(128) if final_bn else Identity(),
            activation(),
            nn.AvgPool2d(6, stride=2, padding=0),
            LinearView(),
        )

        cls = nn.Linear(128, num_classes)
        if cls_spectral_norm:
            nn.utils.spectral_norm(cls)

        self.cls = nn.Sequential(cls)

    def forward(self, x):
        z = self.feat_ext(x)
        y_hat = self.cls(z)

        return y_hat

from torch.utils.data import DataLoader, Subset, Dataset
class my_subset(Dataset):
    """
    Subset of a dataset at specified indices.

    Arguments:
        dataset (Dataset): The whole Dataset
        indices (sequence): Indices in the whole set selected for subset
        labels(sequence) : targets as required for the indices. will be the same length as indices
    """
    def __init__(self, dataset, indices, labels):
        self.dataset = torch.utils.data.Subset(dataset, indices)
        self.targets = torch.tensor(labels)[indices]
    def __getitem__(self, idx):
        image = self.dataset[idx][0]
        target = self.targets[idx]
        return (image, target)

    def __len__(self):
        return len(self.targets)

def collate_fn(it):
    batch_x = []
    batch_y = []
    
    for x, y in it:
        #batch_x.append(torch.stack(x, dim=0))
        batch_x.append(x.unsqueeze(1))
        batch_y.append(torch.tensor([y]*x.shape[0], dtype=torch.long))

    #Convert to tensors
    batch_x = torch.cat(batch_x, 0)
    batch_y = torch.cat(batch_y, 0)
    
    return batch_x, batch_y

num_classes = 10

#Load models
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

#svhn_model_top = SimpleCNN13(num_classes, True, True, False, True).to(device)
#svhn_model_top.load_state_dict(torch.load("/content/drive/MyDrive/TDD/models/svhn_top.pt"))
#svhn_model_top.eval()

#MNIST models
mnist_vanilla = []
for k in range(10):
    mnist_model_vanilla = SimpleCNN_MNIST(num_classes, True).to(device)
    model_path = "/content/drive/MyDrive/TDD/models/" + "mnist_vanilla" + str(k) + ".pt"
    mnist_model_vanilla.load_state_dict(torch.load(model_path, map_location=device))
    mnist_model_vanilla.eval()
    mnist_vanilla.append(mnist_model_vanilla)

#MNIST models
mnist_delayed_11 = []
for k in range(10):
    mnist_model_delayed = SimpleCNN_MNIST(num_classes, True).to(device)
    model_path = "/content/drive/MyDrive/TDD/models/" + "mnist_delayed_11_" + str(k) + ".pt"
    mnist_model_delayed.load_state_dict(torch.load(model_path, map_location=device))
    mnist_model_delayed.eval()
    mnist_delayed_11.append(mnist_model_delayed)

mnist_delayed_09 = []
for k in range(10):
    mnist_model_delayed = SimpleCNN_MNIST(num_classes, True).to(device)
    model_path = "/content/drive/MyDrive/TDD/models/" + "mnist_delayed" + str(k) + ".pt"
    mnist_model_delayed.load_state_dict(torch.load(model_path, map_location=device))
    mnist_model_delayed.eval()
    mnist_delayed_09.append(mnist_model_delayed)

mnist_top = []
#Topological models
for k in range(10):
    mnist_model_top = SimpleCNN_MNIST(num_classes, True).to(device)
    model_path = "/content/drive/MyDrive/TDD/models/" + "mnist_top_" + str(k) + ".pt"
    mnist_model_top.load_state_dict(torch.load(model_path, map_location=device))
    mnist_model_top.eval()
    mnist_top.append(mnist_model_top)

#SVHN Models
svhn_vanilla = []
for k in range(10):
    svhn_model_vanilla = SimpleCNN13(num_classes, True, True, False, True).to(device)
    model_path = "/content/drive/MyDrive/TDD/models/" + "svhn_vanilla_" + str(k) + ".pt"
    svhn_model_vanilla.load_state_dict(torch.load(model_path, map_location=device))
    svhn_model_vanilla.eval()
    svhn_vanilla.append(svhn_model_vanilla)


svhn_delayed = []
for k in range(10):
    svhn_model_delayed = SimpleCNN13(num_classes, True, True, False, True).to(device)
    model_path = "/content/drive/MyDrive/TDD/models/" + "svhn_delayed_" + str(k) + ".pt"
    svhn_model_delayed.load_state_dict(torch.load(model_path, map_location=device))
    svhn_model_delayed.eval()
    svhn_delayed.append(svhn_model_delayed)

svhn_top = []
#Topological models
for k in range(8):
    svhn_model_top = SimpleCNN13(num_classes, True, True, False, True).to(device)
    model_path = "/content/drive/MyDrive/TDD/models/" + "svhn_top_" + str(k) + ".pt"
    svhn_model_top.load_state_dict(torch.load(model_path, map_location=device))
    svhn_model_top.eval()
    svhn_top.append(svhn_model_top)

#Load data splits from data_loader_pickle
import pickle
import os
from pathlib import Path

SPLIT_INDICES_PTH = 'data_train_indices.pickle'

with open(SPLIT_INDICES_PTH, 'br') as fid:
        cache = pickle.load(fid)

I = cache['cifar10_train', 1000]

J = cache['MNIST_train', 250]

K = cache['SVHN_train', 250]

train_data = datasets.MNIST('../data', download=True, train=True, transform=transforms.ToTensor())
test_data = datasets.MNIST('../data', download=True, train=False, transform=transforms.ToTensor())


svhn_train_data = datasets.SVHN('../data', download=True, split='train', transform=transforms.ToTensor())
svhn_test_data = datasets.SVHN('../data', download=True, split='test', transform=transforms.ToTensor())

cifar_train_data = datasets.CIFAR10('../data', download=True, train=True, transform=transforms.ToTensor())
cifar_test_data = datasets.CIFAR10('../data', download=True, train=False, transform=transforms.ToTensor())


#Test data
test_loader = torch.utils.data.DataLoader(
    test_data,
    batch_size=1, shuffle=False, drop_last=False)

svhn_test_loader = torch.utils.data.DataLoader(
    svhn_test_data,
    batch_size=1, shuffle=False, drop_last=False)

cifar_test_loader = torch.utils.data.DataLoader(
    cifar_test_data,
    batch_size=1, shuffle=False, drop_last=False)

runs = 10    
#Take the dataset splits from the pickle (data over 10 runs)
train_data = [my_subset(train_data, indices=i, labels=train_data.targets) for i in J]
svhn_train_data = [my_subset(svhn_train_data, indices=i, labels=svhn_train_data.labels) for i in K]

#Train loaders for each 250-sized training set
dl_train = []
svhn_dl_train = []

for k in range(10):
    dl_train.append(DataLoader(
            train_data[k],
            batch_size=1,
            shuffle=True,
            drop_last=True,
            num_workers=0)) 

    svhn_dl_train.append(DataLoader(
            svhn_train_data[k],
            batch_size=1,
            shuffle=True,
            drop_last=True,
            num_workers=0))

"""# Functions which compute the relevant neural collapse statistics:

*   Distribution of margins
*   Within-class Variability
*   Distance to equiangularity
*   Distance to self-duality
*   Lower bounds on the probabilities Q_k(D_k)






"""

def calc_accuracy(model, data_loader):
    accuracy = 0
    N = 0
    for batch_idx, (x, y) in enumerate(data_loader, start=1):
        x, y = x.to(device), y.to(device)

        # forward pass
        logits = model(x)

        # check if predicted labels are equal to true labels
        predicted_labels = torch.argmax(logits,dim=1)
        accuracy += torch.sum((predicted_labels==y).float()).item()
        N += x.shape[0]
    acc = 100. * accuracy/N
    return acc


'''Returns 3 statistics related to neural collapse:
[within-class variability, distance to equiangularity, distance to self-duality]
'''
def nc_stats(model, data_loader):
    #Measure within-class invariance
    C = 10

    layer = model.feat_ext #Feature space = R^128
    layer.register_forward_hook(hook)

    #Relevant neural collapse statistics
    mean = [0 for _ in range(C)]
    for c in range(C):
        mean[c] = torch.zeros(1, 128).to(device)
    Sw = torch.zeros(128, 128).to(device)
    examples_in_class = [0 for _ in range(C)]

    for computation in ['Mean', 'Cov']:
        for batch_idx, (data, labels) in enumerate(data_loader, start=1):
            data, labels = data.to(device), labels.to(device)
            output = model(data)
            #h = features.value.data.view(data.shape[0],-1) # B CHW
            h = layer.output.view(data.shape[0],-1)
            for c in range(C):
                
                in_class = h[(labels == c).nonzero(as_tuple=True)] #Data in the class c
                examples_in_class[c] += in_class.shape[0] #Add examples to list
                if computation == 'Mean':
                    #Sum over all the data tensors of the same class in examples_in_class
                    if in_class.shape[0] > 0:
                        mean[c] += torch.sum(in_class, 0)
                    
                elif computation == 'Cov':                    
                    cov = torch.zeros(in_class.shape[1], in_class.shape[1]).to(device)                    
                    for i in range(in_class.shape[0]):
                        x_minus_mean = in_class[i,:] - mean[c]
                        cov += torch.matmul(x_minus_mean.T, x_minus_mean)                
                    
                    Sw += cov
                
        # normalize the sum by the number of contributions
        if computation == 'Mean':
            for d in range(C):
                mean[d] = mean[d] / examples_in_class[d]
                M = torch.cat(mean).T
        elif computation == 'Cov':
            Sw = Sw / np.sum(examples_in_class)

    # global mean
    muG = torch.mean(M, dim=1, keepdim=True) # CHW 1

    # between-class covariance

    M_ = M - muG
    Sb = torch.matmul(M_, M_.T) / C

    # NC1: activation collapse
    Sw = Sw.detach().cpu().numpy()
    Sb = Sb.detach().cpu().numpy()
    eigvec, eigval, _ = svds(Sb, k=C-1)

    #Invert all the eigenvalues of eigval to obtain the psuedoinverse of Sb
    eigval_inv = torch.zeros(eigval.shape)
    for i in range(eigval.shape[0]):
        if (eigval[i] != 0):
            eigval_inv[i] = 1.0/eigval[i]
            
    inv_Sb = eigvec @ np.diag(eigval_inv) @ eigvec.T 

    # NC2: convergence of class means to Simplex ETF
    M_norms = torch.norm(M_,  dim=0)

    def coherence(V): 
        G = V.T @ V
        G += torch.ones((C,C),device=device) / (C-1)
        G -= torch.diag(torch.diag(G))
        return torch.norm(G,1).item() / (C*(C-1))

    # NC3: self duality
    W  = model.cls[0].weight

    normalized_M = M_ / torch.norm(M_,'fro')
    normalized_W = W.T / torch.norm(W.T,'fro')
    return [np.trace(Sw @ inv_Sb) / C, (torch.std(M_norms)/torch.mean(M_norms)).item(), (torch.norm(normalized_W - normalized_M)**2).item()]

def calc_margins(model, data_loader):
    all_margins =[]
  
    for batch_idx, (images, labels) in enumerate(data_loader, start=1):
        images, labels = images.to(device), labels.to(device)
        
        # forward pass
        logits = model(images)

        for i in range(len(labels)):
            margins = [(logits[i,labels[i]] - logits[i, l]).cpu().data.numpy() for l in range(len(logits[i])) if l != labels[i]] 
            all_margins.append(margins)

    return all_margins

#Checks if a point y is in the e-radius around z, where e is the min margin of z
def in_circle(y, z, radius):
    dist = torch.norm(y - z, 2)
    return dist < radius

"""Measure the accuracies of the models over 10 training runs."""

svhn_vanilla_acc = []
sum = 0
stdev = 0
for k in range(10):
    svhn_vanilla_acc.append(calc_accuracy(svhn_vanilla[k], svhn_test_loader))
M = np.mean(svhn_vanilla_acc)
print("Mean accuracy: {:.6f}".format(M))
print("Standard deviation: {:.6f}".format(np.sqrt(np.mean([(x - M)**2 for x in svhn_vanilla_acc]))))

svhn_delayed_acc = []
sum = 0
stdev = 0
for k in range(10):
    svhn_delayed_acc.append(calc_accuracy(svhn_delayed[k], svhn_test_loader))
M = np.mean(svhn_delayed_acc)
print("Mean accuracy: {:.6f}".format(M))
print("Standard deviation: {:.6f}".format(np.sqrt(np.mean([(x - M)**2 for x in svhn_delayed_acc]))))

vanilla_acc = []
sum = 0
stdev = 0
for k in range(10):
    vanilla_acc.append(calc_accuracy(mnist_vanilla[k], test_loader))
M = np.mean(vanilla_acc)
print("Mean accuracy: {:.6f}".format(M))
print("Standard deviation: {:.6f}".format(np.sqrt(np.mean([(x - M)**2 for x in vanilla_acc]))))

'''
top_acc = []
sum = 0
stdev = 0
for k in range(10):
    top_acc.append(calc_accuracy(mnist_top[k], test_loader))
M = np.mean(top_acc)
print("Mean accuracy: {:.6f}".format(M))
print("Standard deviation: {:.6f}".format(np.sqrt(np.mean([(x - M)**2 for x in top_acc]))))
'''

delayed_acc = []
sum = 0
stdev = 0
for k in range(10):
    delayed_acc.append(calc_accuracy(mnist_delayed_11[k], test_loader))
M = np.mean(delayed_acc)
print("Mean accuracy: {:.6f}".format(M))
print("Standard deviation: {:.6f}".format(np.sqrt(np.mean([(x - M)**2 for x in delayed_acc]))))

num_classes = 10
feature_dim = 128
'''
Does a projection to the first 2 basis vectors of the simplex ETF
'''
def rep_plot(model, device, layer, data_loader, i, j, name):
    model.eval()
    #data_rep = np.empty((0, 128))
    data_rep = torch.empty((0, 128)).to(device)
    labels = torch.empty((0)).to(device)
    data_pca = []
    colors = ['red','green','blue','purple', 'yellow',
    'magenta','black','orange', 'pink','brown'] #Color code the different classes
    for batch_idx, (x, y) in enumerate(data_loader, start=1):
        x, y = x.to(device), y.to(device)
        # second last layer's output
        model(x)
        features = layer.output

        data_rep = torch.cat((data_rep, features), 0)
        labels = torch.cat((labels, y), 0)

    #Do SVD decomposition
    C = num_classes
    means = svd.SVD_Visualizer(data_rep, labels, C, feature_dim)

    # means
    mu = means.mean()
    mu_G = means.global_mean().unsqueeze(0)

    mu = mu.to(device)
    mu_G = mu_G.to(device)

    mu_c = mu - mu_G #Center class means around global mean

    # weights
    #w = self.model.get_imp_layers()[-1].weight

    # procrustes problem
    J = torch.eye(C) - torch.ones(C)/C
    J = J.to(device)
    u,s,v = torch.svd(J.T @ mu_c)
    R = v @ u.T

    #w_ = w @ R
    mu_c_ = mu_c @ R

    #Project data to the first 2 rows of R
    data_rep_c_ = data_rep @ R
    two_dim_projection = data_rep_c_[:, [i, j]]
    projected_data = two_dim_projection.detach().cpu().numpy()

    plt.figure(figsize=(5,5))
    plt.scatter(projected_data[:,0], projected_data[:,1], c=labels.detach().cpu().numpy(), cmap=matplotlib.colors.ListedColormap(colors))
    plt.xlabel('x')
    plt.ylabel('y')
    plt.title('Representation space of data');
    plt.show()
    #plt.savefig("images_svhn/" + name)

# Save the second last layer's output (i.e. the representation space)
def hook(module, input, output):
    module.output = output

"""Plot the margins of the vanilla models vs. the models with our sliding regularizer."""

for k in range(10):
    plt.figure() 
    all_margins = calc_margins(mnist_vanilla[k], dl_train[k])
    hist, bins_edges = np.histogram(all_margins, bins=50, density=True)
    bin_centers = 0.5*(bins_edges[1:]+bins_edges[:-1]) 
     
    plt.plot(bin_centers, hist, label="vanilla")

    
    all_margins = calc_margins(mnist_delayed_11[k], dl_train[k])
    hist, bins_edges = np.histogram(all_margins, bins=50, density=True)
    bin_centers = 0.5*(bins_edges[1:]+bins_edges[:-1])
    plt.plot(bin_centers, hist, label="delayed")
    plt.legend()

for k in range(10):
    plt.figure() 
    all_margins = calc_margins(svhn_vanilla[k], svhn_dl_train[k])
    hist, bins_edges = np.histogram(all_margins, bins=50, density=True)
    bin_centers = 0.5*(bins_edges[1:]+bins_edges[:-1]) 
     
    plt.plot(bin_centers, hist, label="vanilla")

    
    all_margins = calc_margins(svhn_delayed[k], svhn_dl_train[k])
    hist, bins_edges = np.histogram(all_margins, bins=50, density=True)
    bin_centers = 0.5*(bins_edges[1:]+bins_edges[:-1])
    plt.plot(bin_centers, hist, label="delayed")
    plt.legend()

"""Calculate the 3 neural collapse statistics for the models."""

for k in range(10):
    print("Run " + str(k) + ":")
    print(nc_stats(svhn_vanilla[k], svhn_dl_train[k]))
    print(nc_stats(svhn_delayed[k], svhn_dl_train[k]))

for k in range(10):
    print("Run " + str(k) + ":")
    print(nc_stats(mnist_vanilla[k], dl_train[k]))
    #print(nc_stats(mnist_top[k], dl_train[k]))
    print(nc_stats(mnist_delayed_11[k], dl_train[k]))